{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9bb21b",
   "metadata": {},
   "source": [
    "- identify Document TOC Available or not.\n",
    "- Rule(sorted largest text) + LLM based Document Title finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8601ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\Users\\Aditya Banerjee\\miniconda3\\envs\\docAgentRAG\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Aditya Banerjee\\miniconda3\\envs\\docAgentRAG\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed PDF extraction for all files.\n"
     ]
    }
   ],
   "source": [
    "from logging import config\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "# from marker.schema import BlockTypes\n",
    "\n",
    "'''\n",
    "- output_json: Whether to output the rendered document as JSON. By default, False.\n",
    "- page_range: List of page numbers to process. By default, all pages are processed. eg. [0,1,5-6,...]\n",
    "- ignore_TOC: Whether to ignore the page with Table of Contents page if detected. By default, False.\n",
    "- use_llm: Whether to use LLM to enhance the parsing of the document. By default, False.\n",
    "- ollama_base_url: Base URL for the Ollama LLM service. Default is 'http://localhost:11434'.\n",
    "- llm_service: The LLM service class to use. Default is 'marker.services.ollama.OllamaService'.\n",
    "- disable_ocr: Whether to disable OCR processing. By default, True, if the pdf/doc is not scanned, or in image formatable use OCR. OCR will make the process slow and detect empty spaces, output has no difference.\n",
    "- renderer: The format of the output document can be one of the following ('markdown|pageMarkdown|chunks|pageMarkdown+chunks'). Default is 'pageMarkdown+chunks'.\n",
    "    - for 'markdown', the output dict with keys 'markdown', Dict[str, str].\n",
    "    - for 'pageMarkdown', the output dict with keys 'page_renders' containing 'page numbers' as keys and 'markdown' or 'html' as values, Dict[int, Dict[str, str]].\n",
    "    - for 'chunks', the output dict with keys 'chunks'containing 'page_id' as keys and 'html' as values for the text, Dict[str, Dict[str, Any]].\n",
    "    - for 'json', the output is standard StructuredJSON format for PDFs, Dict[str, Any].\n",
    "    - for 'html', the output is standard HTML format for PDFs, Dict[str, Any].\n",
    "    - for 'pageMarkdown+chunks', the output dict with keys 'page_renders' and 'chunks'.\n",
    "    Output json format contains:\n",
    "        - 'page_structure': List[Dict[str, Any]]  (List of page-wise structure with text and blocks)\n",
    "        - 'page_renders'/'chunks' contain the information of the document.\n",
    "'''\n",
    "\n",
    "paths = [\n",
    "    \"C:\\\\Users\\\\Aditya Banerjee\\\\Documents\\\\Mobius\\\\Deployment\\\\Mobius-DocumentExtraction\\\\PDFs\\\\GNN.pdf\",\n",
    "    \"C:\\\\Users\\\\Aditya Banerjee\\\\Documents\\\\Mobius\\\\Deployment\\\\Mobius-DocumentExtraction\\\\PDFs\\\\Graph_coherence.pdf\"\n",
    "]\n",
    "pdfs = dict()\n",
    "\n",
    "for p in paths:\n",
    "    path = p\n",
    "    converter = PdfConverter(\n",
    "        artifact_dict = create_model_dict(),\n",
    "        config = {\n",
    "            # 'ollama_model': \"llama3.2:latest\",\n",
    "            # \"use_llm\": True,\n",
    "            # \"ollama_base_url\": \"http://ollama-keda.mobiusdtaas.ai\",\n",
    "            # 'llm_service': 'marker.services.ollama.OllamaService',\n",
    "            \"page_range\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "            'disable_ocr': True,\n",
    "            'output_json': True,\n",
    "            \"ignore_TOC\": False,\n",
    "            'ignore_before_TOC': True,\n",
    "            \"renderer\": \"chunks+pageMarkdown\",\n",
    "            \"disable_tqdm\": True,\n",
    "        },\n",
    "        processor_list=[]\n",
    "    )\n",
    "    document = converter.build_document(path)\n",
    "\n",
    "    doc_render = converter.render_document(document)\n",
    "    doc_render.pop('metadata')\n",
    "\n",
    "    pdfs[path] = {\n",
    "        'pdf_id': path.split('\\\\')[-1],\n",
    "        'converter': converter,\n",
    "        'doc_render': doc_render\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Completed PDF extraction for all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4aed214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install dependencies\n",
    "# # Run this cell first to install required packages\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def install_packages():\n",
    "#     packages = [\n",
    "#         \"langgraph\",\n",
    "#         \"langchain-core\",\n",
    "#         \"langchain-community\",\n",
    "#         \"langchain-huggingface\",\n",
    "#         \"langchain-openai\",\n",
    "#         \"faiss-cpu\",\n",
    "#         \"python-dotenv\",\n",
    "#         \"sentence-transformers\"\n",
    "#     ]\n",
    "#     for package in packages:\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0865375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62c9925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    Annotated,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    ")\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Type\n",
    "from marker.schema.document import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83126909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9da607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for C:\\Users\\Aditya Banerjee\\Documents\\Mobius\\Deployment\\Mobius-DocumentExtraction\\PDFs\\GNN.pdf:\n",
      "This research paper provides a comprehensive review of Graph Neural Networks (GNNs), emphasizing their concepts, architectures, techniques, and applications. As deep learning continues to grow, GNNs have emerged as powerful tools for processing graph-structured data, which is critical for understanding relationships in diverse fields such as social networks, biology, and cybersecurity. The authors discuss various GNN models, including Graph Convolution Networks (GCNs), GraphSAGE, and Graph Attention Networks (GATs), highlighting their specific applications and functionalities. The paper also elaborates on the message-passing mechanism used by these models, detailing their strengths and weaknesses across different domains. It further explores common datasets utilized with GNNs, relevant Python libraries, and the overall landscape of GNN research. This review aims to guide future research directions and practical implementations of GNNs by showcasing their capabilities in handling intricate dependencies within graph data. Through this expansive overview, the paper aims to elucidate the significant role GNNs play in tackling challenges that traditional machine learning techniques cannot efficiently solve.\n",
      "\n",
      "Summary for C:\\Users\\Aditya Banerjee\\Documents\\Mobius\\Deployment\\Mobius-DocumentExtraction\\PDFs\\Graph_coherence.pdf:\n",
      "In the paper titled \"Graph-based Local Coherence Modeling,\" the authors, Camille Guinaudeau and Michael Strube, propose a novel, computationally efficient method for modeling local coherence in natural language processing (NLP) using graph structures. They highlight the limitations of existing models, particularly the centering and entity grid models, which suffer from issues like data sparsity and computational complexity. The authors argue that representing entities in a graph format allows for a more effective understanding of local coherence without the drawbacks faced by previous models. \n",
      "\n",
      "They illustrate the effectiveness of their approach by applying it to three key NLP tasks: sentence ordering, summary coherence rating, and readability assessment. Their results indicate that the graph-based model performs comparably to the entity grid model while providing greater flexibility and efficiency. By utilizing centrality measures applied to graph nodes, their model captures coherence across entire texts without the prohibitive computational costs associated with traditional methods. Overall, the paper contributes a new perspective to local coherence modeling, offering a robust alternative to existing methods in various NLP applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate summary of document using first page\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def pdf_summarizer(doc_render: dict) -> str:\n",
    "    '''\n",
    "    Extract the abstract section from the PDF document.\n",
    "    Args:\n",
    "        pdf_filename (str): The path to the PDF file.\n",
    "    Returns:\n",
    "        str: The extracted abstract text.\n",
    "    '''\n",
    "    markdown = doc_render.get('page_renders', [])[0].get('markdown', '')\n",
    "    prompt = HumanMessage(content=f\"Following is the markdown of the first page of a research paper text. Summarize the content of the paper in not more than 200 words:\\n\\n{markdown}\\n\\n'\")\n",
    "    response = llm.invoke([prompt])\n",
    "    return response.content.strip()\n",
    "\n",
    "for path in paths:\n",
    "    summary = pdf_summarizer(pdfs[path]['doc_render'])\n",
    "    pdfs[path]['summary'] = summary\n",
    "    print(f\"Summary for {path}:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046597cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def extract_chunks(doc_render: dict, chunk_size: int=500, chunk_overlap: int=50) -> list:\n",
    "    plain_texts = []\n",
    "    metadata = []\n",
    "    for chunk in doc_render.get('chunks', {}).values():\n",
    "        if (chunk.get('block_type') != 'Text'):\n",
    "            continue\n",
    "        html = chunk.get('html', '').strip()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        plain_text = soup.get_text(separator=' ', strip=True)\n",
    "        chunk_plain_texts = chunk_text(plain_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        for _plain_text in chunk_plain_texts:\n",
    "            plain_texts.append(_plain_text)\n",
    "            metadata.append({\n",
    "                \"id\": f\"/page/{chunk.get('page')}/{chunk.get('block_type')}/{chunk.get('block_id')}\",\n",
    "                \"page\": chunk.get('page')\n",
    "            })\n",
    "    return plain_texts, metadata\n",
    "\n",
    "def find_chunk_by_id(chunk_id: str, doc_render: dict) -> str:\n",
    "    for current_chunk_id, chunk in doc_render.get('chunks', {}).items():\n",
    "        if current_chunk_id == chunk_id:\n",
    "            html = chunk.get('html', '').strip()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            plain_text = soup.get_text(separator=' ', strip=True)\n",
    "            return plain_text\n",
    "    raise ValueError(f\"Chunk with id {chunk_id} not found.\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Build vector store from document chunks\n",
    "for path in paths:\n",
    "    doc_render = pdfs[path]['doc_render']\n",
    "    chunks_text, metadata = extract_chunks(doc_render)\n",
    "    vectorstore = FAISS.from_texts(chunks_text, embeddings, metadatas=metadata) if chunks_text else None\n",
    "    pdfs[path]['vectorstore'] = vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ce25dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node functions\n",
    "import json\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "import ast\n",
    "import math\n",
    "from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "\n",
    "\n",
    "def processor_pipeline(converter: Type[PdfConverter]) -> dict:\n",
    "    for processor in converter.initialize_processors(converter.default_processors):\n",
    "        processor(document)\n",
    "    a = converter.render_document(document)\n",
    "    a.pop('metadata')\n",
    "    return a\n",
    "\n",
    "def markdown_table_to_csv_json(md_text):\n",
    "    # Extract the table section (from first \"|\" line to the last)\n",
    "    table_block = []\n",
    "    capture = False\n",
    "    for line in md_text.splitlines():\n",
    "        if line.strip().startswith(\"|\"):\n",
    "            capture = True\n",
    "            table_block.append(line)\n",
    "        elif capture:\n",
    "            # Stop at first non-table line after capture\n",
    "            break\n",
    "\n",
    "    # Clean lines (remove markdown pipes formatting issues)\n",
    "    clean_lines = [re.sub(r'\\s*\\|\\s*$', '', line.strip()) for line in table_block]\n",
    "    clean_lines = [line.strip(\"|\") for line in clean_lines if line.strip()]\n",
    "\n",
    "    # Split into rows\n",
    "    rows = []\n",
    "    for line in clean_lines:\n",
    "        parts = [re.sub(r'<br>', ' ', cell).strip() for cell in line.split(\"|\")]\n",
    "        rows.append(parts)\n",
    "\n",
    "    # Remove the separator line (---|---|---)\n",
    "    header = rows[0]\n",
    "    rows = [r for r in rows[1:] if not all(set(c.strip()) <= {\"-\", \"\"} for c in r)]\n",
    "\n",
    "    return [header] + rows\n",
    "\n",
    "def generate_multiple_queries(base_query: str, document_summary: str, n: int = 3) -> list:\n",
    "    '''\n",
    "    Generate multiple informative queries based on the base query.\n",
    "    Args:\n",
    "        base_query (str): The base user query.\n",
    "        n (int): Number of queries to generate.\n",
    "    Returns:\n",
    "        list: List of generated queries.\n",
    "    '''\n",
    "    \n",
    "    prompt = HumanMessage(content=f\"Generate {n} different informative queries based on the following base query to retrieve relevant document context\\n\\nOutput format: \\n[\\n    \\\"query1\\\",\\n    \\\"query2\\\",\\n    \\\"query3\\\"\\n]\\n\\nDocument summary: {document_summary}\\n\\nBase Query: {base_query}\\n\\nQueries:\")\n",
    "    response = llm.invoke([prompt])\n",
    "    \n",
    "    queries = ast.literal_eval(response.content)\n",
    "    \n",
    "    return queries[:n]\n",
    "\n",
    "def get_path_from_pdf_id(pdf_id: str) -> str:\n",
    "    pdf_filename = \"\"\n",
    "    for path in paths:\n",
    "        if pdfs[path]['pdf_id'] == pdf_id:\n",
    "            pdf_filename = path\n",
    "            break\n",
    "    if not pdf_filename:\n",
    "        return {'error': 'pdf_id not found.'}\n",
    "    return pdf_filename\n",
    "\n",
    "@tool\n",
    "def extract_text(pdf_id: str, query: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve relevant document context based on the user query. Use informative query to generate better context. \n",
    "    Use the query to find relevant chunks of text from the PDF document.\n",
    "    Args:\n",
    "        pdf_id (str): The id of the PDF file.\n",
    "        query (str): The user query.\n",
    "    Returns:\n",
    "        str: The retrieved document context.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf_filename = get_path_from_pdf_id(pdf_id)\n",
    "    if 'error' in pdf_filename:\n",
    "        return pdf_filename\n",
    "    \n",
    "    vectorstore = pdfs[pdf_filename]['vectorstore']\n",
    "    chunk_set = set([])\n",
    "    # queries = generate_multiple_queries(query, pdfs[pdf_filename]['summary'], n=3)\n",
    "    if vectorstore:\n",
    "        # Create reranker\n",
    "        compressor = FlashrankRerank()\n",
    "        compression_retriever = ContextualCompressionRetriever(\n",
    "            base_compressor=compressor,\n",
    "            base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "        )\n",
    "        for _query in [query]:\n",
    "            docs = compression_retriever.invoke(_query)[:4]\n",
    "            document = []\n",
    "            for doc in docs:\n",
    "                if doc.metadata.get('id') in chunk_set:\n",
    "                    continue\n",
    "                chunk_set.add(doc.metadata.get('id'))\n",
    "                document.append(\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"page\": doc.metadata.get('page'),\n",
    "                            \"Retrieved\": find_chunk_by_id(doc.metadata.get('id'), doc_render)\n",
    "                        }, \n",
    "                        indent=2\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        document = []\n",
    "    \n",
    "    return \"\\n\\n\".join(document)\n",
    "\n",
    "@tool\n",
    "def extract_tables(pdf_id: str, page_num: int = -1) -> list:\n",
    "    '''\n",
    "    Extract all tables from the rendered document for a specific page number.\n",
    "    Args:\n",
    "        pdf_id (str): The id of the PDF file.\n",
    "        page_num (int, optional): The page number to extract tables from. Defaults to -1 (all pages).\n",
    "    Output:\n",
    "        list of extracted tables.\n",
    "    '''\n",
    "    \n",
    "    pdf_filename = get_path_from_pdf_id(pdf_id)\n",
    "    if 'error' in pdf_filename:\n",
    "        return pdf_filename\n",
    "    \n",
    "    converter = pdfs[pdf_filename]['converter']\n",
    "    doc_render = pdfs[pdf_filename]['doc_render']\n",
    "\n",
    "    tables = []\n",
    "    if not converter.processor_list:\n",
    "        doc_render = processor_pipeline(converter)\n",
    "    chunks = doc_render.get('chunks', {})\n",
    "    \n",
    "    for _, content in chunks.items():\n",
    "        if content.get('page') != page_num:\n",
    "            continue\n",
    "        if content.get('block_type') != 'Table':\n",
    "            continue\n",
    "        tables.append({\n",
    "                'page_number': content.get('page'),\n",
    "                'table_data': markdown_table_to_csv_json(content.get('markdown', '')),\n",
    "            })\n",
    "    \n",
    "    return json.dumps(tables)\n",
    "\n",
    "@tool\n",
    "def extract_pdf_abstract(pdf_id: str) -> str:\n",
    "    '''\n",
    "    Extract the abstract section from the PDF document.\n",
    "    Args:\n",
    "        pdf_id (str): The id of the PDF file.\n",
    "    Returns:\n",
    "        str: The extracted abstract text.\n",
    "    '''\n",
    "    \n",
    "    pdf_filename = get_path_from_pdf_id(pdf_id)\n",
    "    if 'error' in pdf_filename:\n",
    "        return pdf_filename\n",
    "    \n",
    "    if pdfs[pdf_filename].get('abstract'):\n",
    "        return pdfs[pdf_filename]['abstract']\n",
    "    doc_render = pdfs[pdf_filename]['doc_render']\n",
    "    markdown = doc_render.get('page_renders', [])[0].get('markdown', '')\n",
    "    prompt = HumanMessage(content=f\"Extract the abstract section from the following markdown text in not more than 200 words:\\n\\n{markdown}\\n\\nIf no abstract is found, respond with 'No abstract found.'\")\n",
    "    response = llm.invoke([prompt])\n",
    "    pdfs[pdf_filename]['abstract'] = response.content.strip()\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "@tool\n",
    "def extract_pdf_page(pdf_id: str, page_num: int) -> str:\n",
    "    '''\n",
    "    Extract the text at page `page_num` from the pdf with the given `pdf_id`. Only call if user specifically requests for a specific page content.\n",
    "    Args:\n",
    "        pdf_id (str): The id of the PDF file.\n",
    "        page_num (int): The page number to extract the abstract from.\n",
    "    Returns:\n",
    "        str: The extracted abstract text.\n",
    "    '''\n",
    "\n",
    "    pdf_filename = get_path_from_pdf_id(pdf_id)\n",
    "    if 'error' in pdf_filename:\n",
    "        return pdf_filename\n",
    "    \n",
    "    doc_render = pdfs[pdf_filename]['doc_render']\n",
    "    markdown = doc_render.get('page_renders', [])[page_num].get('markdown', '')\n",
    "    return markdown\n",
    "\n",
    "@tool\n",
    "def ask_for_user_input(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question to the user to clarify any ambiguities.\n",
    "    Args:\n",
    "        question (str): The question to ask.\n",
    "    Returns:\n",
    "        str: The user's response.\n",
    "    \"\"\"\n",
    "    return input(f\"User Question: {question}\\nYour Answer: \")\n",
    "\n",
    "tools = [extract_text, extract_tables, extract_pdf_abstract, extract_pdf_page, ask_for_user_input]\n",
    "agent = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4ff2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "pdf_info = [\n",
    "    {\n",
    "        'pdf_id': pdfs[path]['pdf_id'],\n",
    "        'summary': pdfs[path]['summary']\n",
    "    }\n",
    "    for path in paths\n",
    "]\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "    # add_messages is a reducer\n",
    "    # See https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers\n",
    "    # pdfs: dict = pdfs\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Define our tool node\n",
    "def tool_node(state: AgentState):\n",
    "    outputs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        outputs.append(\n",
    "            ToolMessage(\n",
    "                content=json.dumps(tool_result),\n",
    "                name=tool_call[\"name\"],\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\"messages\": outputs}\n",
    "\n",
    "# Define the node that calls the model\n",
    "def call_model(\n",
    "    state: AgentState,\n",
    "    config: RunnableConfig,\n",
    "):\n",
    "    # this is similar to customizing the create_react_agent with 'prompt' parameter, but is more flexible\n",
    "    system_prompt = SystemMessage(\n",
    "        \"You are a helpful AI assistant working on helping users to understand information from PDF documents, please respond to the users query to the best of your ability!\" \\\n",
    "        \"Following is some information about the available PDF documents:\\n\\n\" \\\n",
    "        f\"{json.dumps(pdf_info, indent=2)}\\n\\n\" \\\n",
    "        \"Use the PDF information to identify the relevant document and answer the user's query.\" \\\n",
    "    )\n",
    "    response = agent.invoke([system_prompt] + state[\"messages\"], config)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the conditional edge that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af980b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"tools\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Now we can compile and visualize our graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bc6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Can you explain the methodology for implementing graph coherence from the paper it is present on page 3 and 4?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  extract_pdf_page (call_3A2cqVkowH0Nn0AO5ZkGM23f)\n",
      " Call ID: call_3A2cqVkowH0Nn0AO5ZkGM23f\n",
      "  Args:\n",
      "    pdf_id: Graph_coherence.pdf\n",
      "    page_num: 3\n",
      "  extract_pdf_page (call_SlIv1hmYZzJdUEmUle5Vjc74)\n",
      " Call ID: call_SlIv1hmYZzJdUEmUle5Vjc74\n",
      "  Args:\n",
      "    pdf_id: Graph_coherence.pdf\n",
      "    page_num: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: extract_pdf_page\n",
      "\n",
      "\"score of its permutation. In the insertion task, proposed by Elsner and Charniak (2011), we evaluate the ability of our system to retrieve the original position of a sentence previously removed from a document. For this, each sentence is removed in turn and a local coherence score is computed for every possible reinsertion position. The system output is considered as correct if the document associated with the highest local coherence score is the one in which the sentence is reinserted in the correct position.\\n\\nThese two tasks were performed on documents extracted from the English test part of the CoNLL 2012 shared task (Pradhan et al., 2012). This corpus, composed by documents of multiple news sources \\u2013 spoken or written \\u2013 was preferred to the ACCIDENTS and EARTHQUAKES corpora used by Barzilay and Lapata (2008) for two reasons. First, as mentioned by Elsner and Charniak (2008), these corpora use a very constrained style and are not typical of normal informative documents<sup>3</sup> . Second, we want to evaluate the influence of automatically performed coreference resolution in a controlled fashion. The coreference resolution system used performs well on the CoNLL 2012 data. In this dataset, documents composed by the concatenation of differents news articles or too short to have at least 20 permutations were discarded from the corpus. This filtering results in 61 documents composed of 36.1 sentences or 2064 word tokens on average. In both discrimination and insertion, we compare our system against a random baseline where random values are associated with the different orderings.\\n\\n## **4.1.2 Discrimination**\\n\\nAccuracy is used to evaluate the ability of our system to discriminate a document from 20 different permutations. It equals the number of times our system gives the highest score to the original document, divided by the number of comparisons. Since the model can give the same score for a permutation and the original document, we also compute F-measure where recall is *correct/total* and precision equals *correct/decisions*. We test significance using the Student's t-test that can detect significant differences between paired samples. Moreover, as increasing the number of hypotheses\\n\\n<sup>3</sup>Our graph-based model obtains for the discrimination task an accuracy of 0.846 and 0.635 on the ACCIDENTS and EARTHQUAKES datasets, respectively, compared to 0.904 and 0.872 as reported by Barzilay and Lapata (2008).\\n\\nTable 3: Discrimination, reproduced baselines (B&L: Barzilay and Lapata (2008); E&C Elsner and Charniak (2011)) vs. graph-based\\n\\nin a test can also increase the likelihood of witnessing a rare event, and therefore, the chance to reject the null hypothesis when it is true, we use the Bonferroni correction to adjust the increased random likelihood of apparent significance.\\n\\nTable 3 presents the values obtained by three baseline systems when applied to our corpus. Results for the entity grid models described by Barzilay and Lapata (2008) and Elsner and Charniak (2011) are obtained by using Micha Elsner's reimplementation in the Brown Coherence Toolkit<sup>4</sup> . The system was trained on the English training part of the CoNLL 2012 shared task filtered in the same way as the test part.\\n\\nTable 3 also displays the results for our model. These values show that our system performs comparable to the state-of-the-art. Indeed, the difference between our best results and those of Elsner and Charniak are not statistically significant.\\n\\nIn this experiment, distance information is critical. Without it, it is not possible to distinguish between an original document and one of its permutation as both contain the same number and kind of entities. Distance however can detect changes in the distribution of entities within the document as space between entities is significantly modified when sentence order is permuted. When the number of entities \\\"shared\\\" by two sentences is taken into account (P<sup>W</sup> ), the accuracy of our system grows (from 0.830 to 0.871). Table 3 finally shows that syntactic information improves the performance of our system (yet not significantly) and gives the best results (PAcc).\\n\\nWe also evaluated the influence of coreference resolution on the performance of our system. Us-\\n\\n<sup>4</sup>https://bitbucket.org/melsner/\\n\\nbrowncoherence; B&L is Elsner's \\\"baseline entity grid\\\" (command line option '-n'), E&C is Elsner's \\\"extended entity grid\\\" ('-f')\"\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The methodology for implementing graph coherence, as discussed in the paper \"Graph-based Local Coherence Modeling\" by Camille Guinaudeau and Michael Strube, involves constructing a graph-based representation of local coherence in natural language texts. Here are the key points from pages 3 and 4 that outline this methodology:\n",
      "\n",
      "### 1. Graph Representation\n",
      "- **Projection Graphs**: The authors utilize projection graphs to represent the relationships between sentences based on discourse entities. Weights are assigned to edges in the graph to reflect the strength of connections between sentences, adjusting for distance between entities.\n",
      "\n",
      "### 2. Coherence Measurement\n",
      "- **Average Outdegree**: The local coherence of a text \\(T\\) is measured by calculating the average outdegree of the projection graph \\(P\\). The average outdegree allows evaluation of how connected a sentence is to other sentences in terms of discourse entities. This measurement is computationally efficient (complexity of \\(O(N^2)\\), where \\(N\\) is the number of sentences).\n",
      "  \n",
      "  \\[\n",
      "  \\text{LocalCoherence}(T) = \\text{AvgOutDegree}(P) = \\frac{1}{N} \\sum_{i=1}^{N} \\text{OutDegree}(s_i)\n",
      "  \\]\n",
      "  where \\(\\text{OutDegree}(s_i)\\) is the sum of weights associated with the edges leaving sentence \\(s_i\\).\n",
      "\n",
      "### 3. Entity Definition and Coreference Resolution\n",
      "- All nouns in a document are treated as discourse entities, including those that don't head noun phrases. A coreference resolution system is employed to treat coreferent entities as the same discourse entity for coherence modeling.\n",
      "\n",
      "### 4. Syntactic Weight Assignments\n",
      "- Syntactic weights are assigned to edges based on linguistic intuitiveness; for example, subjects are assigned more weight than objects. The study finds that adhering to a weight hierarchy (subjects > objects > others) correlates with accurate coherence assessments.\n",
      "\n",
      "### 5. Experimental Evaluation\n",
      "- The system undergoes three main experimental tasks to evaluate its ability to estimate local coherence: \n",
      "  1. **Sentence Ordering**: Measures how well the system can determine the original order of sentences in a document.\n",
      "  2. **Summary Coherence Rating**: Assesses how well the model evaluates coherence in shorter texts/summaries.\n",
      "  3. **Readability Assessment**: Evaluates the readability of a text based on coherence measures.\n",
      "\n",
      "### 6. Flexibility and Adaptability\n",
      "- The model's parameters can be optimized for different tasks by fine-tuning on development datasets, allowing it to be tailored for specific applications in coherence evaluation.\n",
      "\n",
      "This methodology provides a novel approach to modeling coherence in natural language processing, demonstrating significant potential improvements over traditional methods.\n"
     ]
    }
   ],
   "source": [
    "# Helper function for formatting the stream nicely\n",
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "inputs = {\"messages\": [(\"user\", \"Can you explain the methodology for implementing graph coherence from the paper it is present on page 3 and 4?\")]}\n",
    "print_stream(graph.stream(inputs, stream_mode=\"values\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docAgentRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
